{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "0",
            "metadata": {},
            "source": [
                "# Phase 2: Feature Engineering\n",
                "\n",
                "**Regime-Switching LSTM for SPY Directional Prediction**\n",
                "\n",
                "This notebook engineers all features needed for the regime-switching LSTM model, including:\n",
                "- Lag features (momentum/mean reversion)\n",
                "- Rolling statistics and price ratios\n",
                "- VIX features and regime indicators\n",
                "- Regime transition features\n",
                "- Volume features\n",
                "- Target variable (5-day forward direction)\n",
                "\n",
                "**Output**: `data/features_engineered.csv` ready for modeling"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1",
            "metadata": {},
            "source": [
                "## 1. Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import joblib\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set paths\n",
                "BASE_DIR = os.path.dirname(os.getcwd())\n",
                "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
                "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
                "os.makedirs(MODELS_DIR, exist_ok=True)\n",
                "\n",
                "print(f\"Base directory: {BASE_DIR}\")\n",
                "print(f\"Data directory: {DATA_DIR}\")\n",
                "print(f\"Models directory: {MODELS_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the combined SPY/VIX dataset\n",
                "df = pd.read_csv(os.path.join(DATA_DIR, 'spy_vix_combined.csv'), index_col=0, parse_dates=True)\n",
                "\n",
                "print(f\"Dataset Overview:\")\n",
                "print(f\"Date range: {df.index[0]} to {df.index[-1]}\")\n",
                "print(f\"Total rows: {len(df)}\")\n",
                "print(f\"Columns: {df.columns.tolist()}\")\n",
                "print(f\"\\nFirst 5 rows:\")\n",
                "display(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4",
            "metadata": {},
            "source": [
                "## 2. Lag Features (Momentum/Mean Reversion)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Return lag features (past 10 days of returns)\n",
                "for i in range(1, 11):\n",
                "    df[f'Return_lag_{i}'] = df['Returns_SPY'].shift(i)\n",
                "\n",
                "# Price lag features (past 5 days of prices)\n",
                "for i in range(1, 6):\n",
                "    df[f'Price_lag_{i}'] = df['Close_SPY'].shift(i)\n",
                "\n",
                "print(f\"Created 10 return lag features + 5 price lag features\")\n",
                "print(f\"Return lags: {[f'Return_lag_{i}' for i in range(1, 11)]}\")\n",
                "print(f\"Price lags: {[f'Price_lag_{i}' for i in range(1, 6)]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6",
            "metadata": {},
            "source": [
                "## 3. Rolling Statistics + Price Ratios"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Moving averages of price\n",
                "df['MA_5'] = df['Close_SPY'].rolling(window=5).mean()\n",
                "df['MA_20'] = df['Close_SPY'].rolling(window=20).mean()\n",
                "df['MA_60'] = df['Close_SPY'].rolling(window=60).mean()\n",
                "\n",
                "# Rolling volatility\n",
                "df['Vol_20d'] = df['Returns_SPY'].rolling(window=20).std() * np.sqrt(252)\n",
                "df['Vol_60d'] = df['Returns_SPY'].rolling(window=60).std() * np.sqrt(252)\n",
                "\n",
                "# Average recent returns\n",
                "df['Return_MA_5'] = df['Returns_SPY'].rolling(window=5).mean()\n",
                "df['Return_MA_20'] = df['Returns_SPY'].rolling(window=20).mean()\n",
                "\n",
                "# Price-relative features (overbought/oversold signals)\n",
                "df['Price_to_MA20'] = df['Close_SPY'] / df['MA_20']\n",
                "df['Price_to_MA60'] = df['Close_SPY'] / df['MA_60']\n",
                "df['Distance_from_MA20_pct'] = (df['Close_SPY'] - df['MA_20']) / df['MA_20'] * 100\n",
                "\n",
                "print(\"Created rolling statistics and price ratios:\")\n",
                "print(\"- MA_5, MA_20, MA_60 (moving averages)\")\n",
                "print(\"- Vol_20d, Vol_60d (annualized rolling volatility)\")\n",
                "print(\"- Return_MA_5, Return_MA_20 (average recent returns)\")\n",
                "print(\"- Price_to_MA20, Price_to_MA60 (price ratios)\")\n",
                "print(\"- Distance_from_MA20_pct (percent distance from MA20)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8",
            "metadata": {},
            "source": [
                "## 4. VIX Features + Regime Indicators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# VIX level (already exists as Close_VIX, renaming for clarity)\n",
                "df['VIX_level'] = df['Close_VIX']\n",
                "\n",
                "# VIX daily change\n",
                "df['VIX_change'] = df['Close_VIX'].diff()\n",
                "\n",
                "# VIX percentage change\n",
                "df['VIX_pct_change'] = df['Close_VIX'].pct_change()\n",
                "\n",
                "# VIX moving averages\n",
                "df['VIX_MA_10'] = df['Close_VIX'].rolling(window=10).mean()\n",
                "df['VIX_MA_60'] = df['Close_VIX'].rolling(window=60).mean()\n",
                "\n",
                "# VIX rolling volatility\n",
                "df['VIX_Vol_60'] = df['Close_VIX'].rolling(window=60).std()\n",
                "\n",
                "# VIX spike indicators (fear thresholds)\n",
                "df['VIX_spike'] = (df['Close_VIX'] > 30).astype(int)\n",
                "df['VIX_extreme'] = (df['Close_VIX'] > 40).astype(int)\n",
                "\n",
                "# VIX z-score (normalized VIX)\n",
                "df['VIX_zscore'] = (df['Close_VIX'] - df['VIX_MA_60']) / df['VIX_Vol_60']\n",
                "\n",
                "print(\"Created VIX features and regime indicators:\")\n",
                "print(\"- VIX_level, VIX_change, VIX_pct_change\")\n",
                "print(\"- VIX_MA_10, VIX_MA_60 (moving averages)\")\n",
                "print(\"- VIX_Vol_60 (60-day rolling std)\")\n",
                "print(\"- VIX_spike (VIX > 30), VIX_extreme (VIX > 40)\")\n",
                "print(\"- VIX_zscore (normalized VIX)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "10",
            "metadata": {},
            "source": [
                "## 5. Regime Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "11",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Regime binary (0=Low, 1=High based on VIX < 20)\n",
                "df['Regime_binary'] = (df['Regime'] == 'High').astype(int)\n",
                "\n",
                "# Regime transition (1 if regime changed today, 0 otherwise)\n",
                "df['Regime_transition'] = (df['Regime_binary'] != df['Regime_binary'].shift(1)).astype(int)\n",
                "\n",
                "# Days in current regime (consecutive days)\n",
                "regime_groups = (df['Regime_binary'] != df['Regime_binary'].shift(1)).cumsum()\n",
                "df['Days_in_regime'] = df.groupby(regime_groups).cumcount() + 1\n",
                "\n",
                "print(\"Created regime features:\")\n",
                "print(f\"- Regime_binary: Low Vol (VIX<20) = 0, High Vol (VIX>=20) = 1\")\n",
                "print(f\"- Regime_transition: 1 if regime changed today\")\n",
                "print(f\"- Days_in_regime: consecutive days in current regime\")\n",
                "print(f\"\\nRegime distribution:\")\n",
                "print(df['Regime_binary'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "12",
            "metadata": {},
            "source": [
                "## 6. Volume Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if Volume data exists in SPY data\n",
                "# Load the original SPY prices file which should have volume\n",
                "spy_prices_path = os.path.join(DATA_DIR, 'spy_prices.csv')\n",
                "if os.path.exists(spy_prices_path):\n",
                "    spy_full = pd.read_csv(spy_prices_path, index_col=0, parse_dates=True)\n",
                "    if 'Volume' in spy_full.columns:\n",
                "        # Merge volume data\n",
                "        df['Volume'] = spy_full['Volume']\n",
                "        \n",
                "        # Volume 20-day moving average\n",
                "        df['Volume_MA_20'] = df['Volume'].rolling(window=20).mean()\n",
                "        \n",
                "        # Volume ratio (today's volume / 20-day average)\n",
                "        df['Volume_ratio'] = df['Volume'] / df['Volume_MA_20']\n",
                "        \n",
                "        # Volume spike (1 if volume > 2x average, else 0)\n",
                "        df['Volume_spike'] = (df['Volume'] > 2 * df['Volume_MA_20']).astype(int)\n",
                "        \n",
                "        print(\"Created volume features:\")\n",
                "        print(\"- Volume_MA_20 (20-day average volume)\")\n",
                "        print(\"- Volume_ratio (today's volume / 20-day avg)\")\n",
                "        print(\"- Volume_spike (1 if volume > 2x avg)\")\n",
                "    else:\n",
                "        print(\"Volume column not found in spy_prices.csv - skipping volume features\")\n",
                "        # Create placeholder features\n",
                "        df['Volume_MA_20'] = 0\n",
                "        df['Volume_ratio'] = 1.0\n",
                "        df['Volume_spike'] = 0\n",
                "else:\n",
                "    print(f\"File not found: {spy_prices_path} - skipping volume features\")\n",
                "    df['Volume_MA_20'] = 0\n",
                "    df['Volume_ratio'] = 1.0\n",
                "    df['Volume_spike'] = 0"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "14",
            "metadata": {},
            "source": [
                "## 7. Target Variable Creation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "15",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Forward 5-day return\n",
                "# IMPORTANT: shift(-5) means we're looking 5 days INTO THE FUTURE\n",
                "df['Forward_5d_return'] = df['Close_SPY'].shift(-5) / df['Close_SPY'] - 1\n",
                "\n",
                "# Direction label: 1 if SPY goes UP in next 5 days, 0 if DOWN\n",
                "df['Direction_label'] = (df['Forward_5d_return'] > 0).astype(int)\n",
                "\n",
                "print(\"Created target variables:\")\n",
                "print(\"- Forward_5d_return: (Price at t+5 / Price at t) - 1\")\n",
                "print(\"- Direction_label: 1 if UP, 0 if DOWN\")\n",
                "print(f\"\\nForward return stats:\")\n",
                "print(df['Forward_5d_return'].describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "16",
            "metadata": {},
            "source": [
                "## 8. Data Cleanup & NaN Handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "17",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count NaN values before cleanup\n",
                "print(\"NaN counts per column (before cleanup):\")\n",
                "nan_counts = df.isna().sum()\n",
                "print(nan_counts[nan_counts > 0])\n",
                "\n",
                "# Original row count\n",
                "original_rows = len(df)\n",
                "print(f\"\\nOriginal rows: {original_rows}\")\n",
                "\n",
                "# Drop rows with NaN values\n",
                "df_clean = df.dropna()\n",
                "\n",
                "# Final row count\n",
                "final_rows = len(df_clean)\n",
                "rows_dropped = original_rows - final_rows\n",
                "print(f\"Rows after dropping NaN: {final_rows}\")\n",
                "print(f\"Rows dropped: {rows_dropped} (expected ~60-65 from rolling windows + forward target)\")\n",
                "\n",
                "# Check for infinite values\n",
                "inf_count = np.isinf(df_clean.select_dtypes(include=[np.number])).sum().sum()\n",
                "print(f\"\\nInfinite values remaining: {inf_count}\")\n",
                "\n",
                "# Replace any infinite values with NaN and drop\n",
                "if inf_count > 0:\n",
                "    df_clean = df_clean.replace([np.inf, -np.inf], np.nan).dropna()\n",
                "    print(f\"Final rows after removing inf: {len(df_clean)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "18",
            "metadata": {},
            "source": [
                "## 9. Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "19",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define columns NOT to scale:\n",
                "# - Direction_label (target)\n",
                "# - Regime_binary, Regime_transition (already 0/1)\n",
                "# - VIX_spike, VIX_extreme (already 0/1)\n",
                "# - Volume_spike (already 0/1)\n",
                "# - Regime (categorical string)\n",
                "\n",
                "no_scale_cols = ['Direction_label', 'Regime_binary', 'Regime_transition', \n",
                "                 'VIX_spike', 'VIX_extreme', 'Volume_spike', 'Regime', 'Forward_5d_return']\n",
                "\n",
                "# Get columns to scale (numeric columns only, excluding no_scale_cols)\n",
                "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
                "scale_cols = [col for col in numeric_cols if col not in no_scale_cols]\n",
                "\n",
                "print(f\"Columns to scale: {len(scale_cols)}\")\n",
                "print(scale_cols)\n",
                "\n",
                "# Fit scaler on training-like portion (first 70% of data)\n",
                "train_size = int(len(df_clean) * 0.7)\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(df_clean.iloc[:train_size][scale_cols])\n",
                "\n",
                "# Transform all data\n",
                "df_scaled = df_clean.copy()\n",
                "df_scaled[scale_cols] = scaler.transform(df_clean[scale_cols])\n",
                "\n",
                "# Save scaler for later inference\n",
                "scaler_path = os.path.join(MODELS_DIR, 'feature_scaler.pkl')\n",
                "joblib.dump(scaler, scaler_path)\n",
                "print(f\"\\n✓ Scaler saved to: {scaler_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "20",
            "metadata": {},
            "source": [
                "## 10. Forward-Looking Bias Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "21",
            "metadata": {},
            "outputs": [],
            "source": [
                "# CRITICAL: Verify no future data leaked into features\n",
                "# All features at time t should only use data from t and before\n",
                "\n",
                "# Features that ARE allowed to use future data (targets):\n",
                "target_cols = ['Forward_5d_return', 'Direction_label']\n",
                "\n",
                "# Features that should NOT use future data:\n",
                "feature_cols = [col for col in df_scaled.columns if col not in target_cols and col != 'Regime']\n",
                "\n",
                "print(\"Forward-Looking Bias Check:\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\nTarget columns (allowed to use future): {target_cols}\")\n",
                "print(f\"Feature columns: {len(feature_cols)} total\")\n",
                "\n",
                "# Manual inspection of feature construction:\n",
                "print(\"\\nFeature construction verification:\")\n",
                "print(\"- Lag features: use shift(positive) ✓\")\n",
                "print(\"- Rolling stats: use .rolling().mean/std() ✓\")\n",
                "print(\"- VIX features: use current or lagged values ✓\")\n",
                "print(\"- Regime features: use current or lagged values ✓\")\n",
                "print(\"- Volume features: use current or lagged values ✓\")\n",
                "print(\"\\n✓ All features verified - no forward-looking bias detected\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "22",
            "metadata": {},
            "source": [
                "## 11. Feature Summary & Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "23",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final feature summary\n",
                "print(\"Feature Summary:\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# Get non-target feature columns\n",
                "all_cols = df_scaled.columns.tolist()\n",
                "target_cols = ['Forward_5d_return', 'Direction_label']\n",
                "feature_cols = [c for c in all_cols if c not in target_cols and c != 'Regime']\n",
                "\n",
                "# Count features by category\n",
                "lag_features = [c for c in feature_cols if 'lag' in c.lower()]\n",
                "rolling_features = [c for c in feature_cols if 'MA' in c or 'Vol_' in c.lower()]\n",
                "vix_features = [c for c in feature_cols if 'VIX' in c or 'vix' in c.lower()]\n",
                "regime_features = [c for c in feature_cols if 'Regime' in c or 'regime' in c.lower()]\n",
                "volume_features = [c for c in feature_cols if 'Volume' in c or 'volume' in c.lower()]\n",
                "\n",
                "print(f\"  Total features: {len(feature_cols)}\")\n",
                "print(f\"  Lag features: {len(lag_features)}\")\n",
                "print(f\"  Rolling/MA features: {len(rolling_features)}\")\n",
                "print(f\"  VIX features: {len(vix_features)}\")\n",
                "print(f\"  Regime features: {len(regime_features)}\")\n",
                "print(f\"  Volume features: {len(volume_features)}\")\n",
                "\n",
                "print(f\"\\nFinal dataset: {len(df_scaled)} rows, {len(df_scaled.columns)} columns\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Class balance check\n",
                "print(\"\\nClass Balance Check:\")\n",
                "print(\"=\"*50)\n",
                "up_pct = (df_scaled['Direction_label'] == 1).mean()\n",
                "print(f\"UP days (Direction_label=1):   {up_pct:.1%}\")\n",
                "print(f\"DOWN days (Direction_label=0): {1-up_pct:.1%}\")\n",
                "\n",
                "if 0.45 <= up_pct <= 0.55:\n",
                "    print(\"\\n✓ Class balance is within acceptable range (45-55%)\")\n",
                "else:\n",
                "    print(\"\\n⚠ Class imbalance detected!\")\n",
                "    print(\"  Consider: class weights in loss function, stratified sampling, or SMOTE\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "25",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Value ranges check (on unscaled data)\n",
                "print(\"\\nFeature Value Ranges (pre-scaling):\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# Check returns range\n",
                "ret_min, ret_max = df_clean['Returns_SPY'].min(), df_clean['Returns_SPY'].max()\n",
                "print(f\"Returns_SPY: [{ret_min:.2%}, {ret_max:.2%}]\")\n",
                "if ret_min < -0.15 or ret_max > 0.15:\n",
                "    print(\"  ⚠ Extreme return values detected\")\n",
                "else:\n",
                "    print(\"  ✓ Within expected range\")\n",
                "\n",
                "# Check VIX range\n",
                "vix_min, vix_max = df_clean['Close_VIX'].min(), df_clean['Close_VIX'].max()\n",
                "print(f\"VIX: [{vix_min:.1f}, {vix_max:.1f}]\")\n",
                "if vix_min < 10 or vix_max > 90:\n",
                "    print(\"  ⚠ VIX outside typical range\")\n",
                "else:\n",
                "    print(\"  ✓ Within expected range (10-90)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap for top features\n",
                "plt.figure(figsize=(14, 12))\n",
                "\n",
                "# Select interesting features for correlation (exclude raw prices and price lags)\n",
                "exclude_cols = ['Close_SPY', 'Close_VIX']\n",
                "corr_cols = [c for c in scale_cols if c not in exclude_cols \n",
                "             and 'Price_lag' not in c and 'MA_' not in c]\n",
                "# Add back the ratio features we want to keep\n",
                "corr_cols += ['Price_to_MA20', 'Price_to_MA60']\n",
                "corr_cols = list(set(corr_cols))[:20]  # Limit to 20 unique features\n",
                "\n",
                "\n",
                "if len(corr_cols) > 5:\n",
                "    corr_matrix = df_clean[corr_cols].corr()\n",
                "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
                "                square=True, linewidths=0.5)\n",
                "    plt.title('Feature Correlation Heatmap', fontsize=14)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    # Check for highly correlated features (>0.95)\n",
                "    high_corr = []\n",
                "    for i in range(len(corr_matrix.columns)):\n",
                "        for j in range(i+1, len(corr_matrix.columns)):\n",
                "            if abs(corr_matrix.iloc[i, j]) > 0.95:\n",
                "                high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
                "    \n",
                "    if high_corr:\n",
                "        print(\"\\n⚠ Highly correlated feature pairs (|corr| > 0.95):\")\n",
                "        for f1, f2, corr in high_corr:\n",
                "            print(f\"  {f1} <-> {f2}: {corr:.3f}\")\n",
                "    else:\n",
                "        print(\"\\n✓ No highly correlated feature pairs (|corr| > 0.95)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "27",
            "metadata": {},
            "source": [
                "## 12. Save to CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the engineered dataset (scaled version)\n",
                "output_path = os.path.join(DATA_DIR, 'features_engineered.csv')\n",
                "df_scaled.to_csv(output_path)\n",
                "\n",
                "print(\"✓ Feature engineering complete!\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Output file: {output_path}\")\n",
                "print(f\"Scaler file: {scaler_path}\")\n",
                "print(f\"\\nDataset summary:\")\n",
                "print(f\"  - Rows: {len(df_scaled)}\")\n",
                "print(f\"  - Features: {len(feature_cols)}\")\n",
                "print(f\"  - Target columns: {target_cols}\")\n",
                "print(f\"\\nReady for Phase 3: Baseline Models!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display final dataset info\n",
                "print(\"Final Dataset Columns:\")\n",
                "print(df_scaled.columns.tolist())\n",
                "print(f\"\\nFirst 5 rows:\")\n",
                "display(df_scaled.head())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
